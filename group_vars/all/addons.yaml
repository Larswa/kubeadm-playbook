#####
# ADDONS which are not helm charts and not related to helm charts# uncomment/ add the desired ones.
# These are run by post_deploy (right after cluster deploy), before the helm role starts.
# k8s_addons_urls:
# - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/node-problem-detector/npd.yaml # rbac ready
#####

#####
# ADDONS related to helm, and called from the helm role (and after the k8s_addons_urls)
# The place to apply crds, webhooks, rbac, etc
pre_helm_manifests:
## cert-manager crds:
#- https://raw.githubusercontent.com/jetstack/cert-manager/release-0.14/deploy/manifests/00-crds.yaml
- https://github.com/jetstack/cert-manager/releases/download/v1.1.0/cert-manager.crds.yaml

###################
## HELM & CHARTS ##
###################
helm:
  helm_version: v3.4.1 # or "latest" #https://github.com/kubernetes/helm/releases
  install_script_url: 'https://github.com/kubernetes/helm/raw/master/scripts/get-helm-3' 
  repos: ## stable repo is installed by helm by default, no need for its entry here, add only new ones
  - { name: center, url: 'https://repo.chartcenter.io' } # https://rimusz.net/migrating-to-ingress-nginx
  #- { name: stable, url: 'http://kubernetes-charts.storage.googleapis.com' }  # for metrics server till this is fixed: https://github.com/kubernetes-sigs/metrics-server/pull/606 and kube-state-metrics (see https://github.com/kubernetes/kube-state-metrics/pull/1237 )
  - { name: stable, url: 'https://charts.helm.sh/stable' }  # for metrics server till this is fixed: https://github.com/kubernetes-sigs/metrics-server/pull/606 and kube-state-metrics (see https://github.com/kubernetes/kube-state-metrics/pull/1237 )
  - { name: grafana, url: 'https://grafana.github.io/helm-charts' }  # for grafana/grafana which is called by kube-prometehus-stack  #https://github.com/grafana/helm-charts/tree/main/charts/grafana
  - { name: prometheus-community, url: 'https://prometheus-community.github.io/helm-charts' } # for prometheus-community/kube-prometheus-stack and prometheus-community/prometheus-node-exporter # calls grafana/grafana and stable/kube-state-metrics
  - { name: ingress-nginx, url: 'https://kubernetes.github.io/ingress-nginx' }
  - { name: kured, url: 'https://weaveworks.github.io/kured' }
  - { name: incubator, url: 'http://storage.googleapis.com/kubernetes-charts-incubator' }
  - { name: jetstack, url: 'https://charts.jetstack.io' } # cert-manager
#  - { name: funkypenguin, url: 'https://funkypenguin.github.io/helm-kubernetes-dashboard' } #dashboard 2.0 (till PR kubernetes/dashboard#4502 merged in official repo)
  - { name: kubernetes-dashboard, url: 'https://kubernetes.github.io/dashboard/' }
  - { name: flexkube, url: 'https://flexkube.github.io/charts' } # calico helm chart till official will be ready: https://github.com/projectcalico/calico/issues/717 and https://github.com/projectcalico/calico/tree/master/_includes/charts/calico # plan to move to tigera-opearator
  - { name: rook-release, url: 'https://charts.rook.io/release' }
  packages_list: # when not defined, namespace defaults to "default" namespace
  # use "--wait" in the options section if you want to wait till min. pods are up.
  ### List helm charts you wish pre-installed every time cluster is deployed:

###########################
## Networking: calico or other options
## Calico overlay network #
###########################
    - { name: calico, repo: flexkube/calico, namespace: kube-system, options: '--set podCIDR="" --set typha.image.registry={{ images_repo | default ("docker.io") }} --set node.image.registry={{ images_repo | default ("docker.io") }}' } # if autodetect does not work, use '--set podCIDR={{ POD_NETWORK_CIDR }}' } # if not needed, also add --skip-crds

########################
## Monitoring: prometheus, using coreos's prometheus-operator, which includes: grafana, alertmanager, prometheus.
## PROMETHEUS Operator #
########################
# k delete crd alertmanagers.monitoring.coreos.com prometheusrules.monitoring.coreos.com servicemonitors.monitoring.coreos.com prometheuses.monitoring.coreos.com podmonitors.monitoring.coreos.com
# helm install --name prometheus stable/prometheus-operator --namespace monitoring 
    - { name: prometheus, repo: prometheus-community/kube-prometheus-stack, namespace: monitoring, options: '--set prometheusOperator.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheusOperator.tolerations[0].effect=NoSchedule,prometheusOperator.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheusOperator.tolerations[1].effect=PreferNoSchedule,prometheusOperator.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus.prometheusSpec.tolerations[0].effect=NoSchedule,prometheus.prometheusSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.tolerations[1].effect=PreferNoSchedule,prometheus.prometheusSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set alertmanager.alertmanagerSpec.tolerations[0].effect=NoSchedule,alertmanager.alertmanagerSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.tolerations[1].effect=PreferNoSchedule,alertmanager.alertmanagerSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set grafana.nodeSelector."node\-role\.kubernetes\.io/infra=" --set grafana.tolerations[0].effect=NoSchedule,grafana.tolerations[0].key="node-role.kubernetes.io/infra" --set grafana.tolerations[1].effect=PreferNoSchedule,grafana.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.ingress.enabled=True --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} --set prometheusOperator.image.repository={{ images_repo | default ("quay.io") }}/prometheus-operator/prometheus-operator --set prometheusOperator.configmapReloadImage.repository={{ images_repo | default ("docker.io") }}/jimmidyson/configmap-reload --set prometheusOperator.prometheusConfigReloaderImage.repository={{ images_repo | default ("quay.io") }}/prometheus-operator/prometheus-config-reloader --set prometheus.prometheusSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/prometheus --set alertmanager.alertmanagerSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/alertmanager --set prometheusOperator.hyperkubeImage.repository={{ images_repo | default ("k8s.gcr.io") }}/hyperkube --set grafana.image.repository={{ images_repo | default ("docker.io") }}/grafana/grafana --set kube-state-metrics.image.repository={{ images_repo | default ("quay.io") }}/coreos/kube-state-metrics --set kube-state-metrics.tolerations[0].effect=NoSchedule,kube-state-metrics.tolerations[0].key="node-role.kubernetes.io/infra" --set kube-state-metrics.tolerations[1].effect=PreferNoSchedule,kube-state-metrics.tolerations[1].key="node-role.kubernetes.io/infra" --set kube-state-metrics.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus-node-exporter.image.repository={{ images_repo | default ("quay.io") }}/prometheus/node-exporter ' }
    # - { name: prometheus, repo: stable/prometheus-operator, namespace: monitoring, options: '--set prometheusOperator.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheusOperator.tolerations[0].effect=NoSchedule,prometheusOperator.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheusOperator.tolerations[1].effect=PreferNoSchedule,prometheusOperator.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus.prometheusSpec.tolerations[0].effect=NoSchedule,prometheus.prometheusSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.tolerations[1].effect=PreferNoSchedule,prometheus.prometheusSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set alertmanager.alertmanagerSpec.tolerations[0].effect=NoSchedule,alertmanager.alertmanagerSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.tolerations[1].effect=PreferNoSchedule,alertmanager.alertmanagerSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set grafana.nodeSelector."node\-role\.kubernetes\.io/infra=" --set grafana.tolerations[0].effect=NoSchedule,grafana.tolerations[0].key="node-role.kubernetes.io/infra" --set grafana.tolerations[1].effect=PreferNoSchedule,grafana.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.ingress.enabled=True --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} --set prometheusOperator.image.repository={{ images_repo | default ("quay.io") }}/coreos/prometheus-operator --set prometheusOperator.configmapReloadImage.repository={{ images_repo | default ("docker.io") }}/jimmidyson/configmap-reload --set prometheusOperator.prometheusConfigReloaderImage.repository={{ images_repo | default ("quay.io") }}/coreos/prometheus-config-reloader --set prometheus.prometheusSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/prometheus --set alertmanager.alertmanagerSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/alertmanager --set prometheusOperator.hyperkubeImage.repository={{ images_repo | default ("k8s.gcr.io") }}/hyperkube --set grafana.image.repository={{ images_repo | default ("docker.io") }}/grafana/grafana --set kube-state-metrics.image.repository={{ images_repo | default ("quay.io") }}/coreos/kube-state-metrics --set kube-state-metrics.tolerations[0].effect=NoSchedule,kube-state-metrics.tolerations[0].key="node-role.kubernetes.io/infra" --set kube-state-metrics.tolerations[1].effect=PreferNoSchedule,kube-state-metrics.tolerations[1].key="node-role.kubernetes.io/infra" --set kube-state-metrics.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus-node-exporter.image.repository={{ images_repo | default ("quay.io") }}/prometheus/node-exporter ' }

    #- { name: prometheus, repo: stable/prometheus-operator, namespace: monitoring, options: '--set prometheus.ingress.enabled=True --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} ' }

################
#### Heapster ##
################
# Will be deprecated as soon as we can use metrics-server from dashboard 2.0 helm chart will be officially released.
#    - { name: heapster, repo: stable/heapster, namespace: kube-system, options: '--set service.nameOverride=heapster,rbac.create=true --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set image.repository={{ images_repo | default ("k8s.gcr.io") }}/heapster-{{ HOST_ARCH }} --set resizer.image.repository={{ images_repo | default ("k8s.gcr.io") }}/addon-resizer --set resizer.enabled=False ' }
# --set resizer.enabled=False ->> this is not working with taints due to some bug, thefore disabling it for now.
#    - { name: heapster, repo: stable/heapster, namespace: kube-system, options: '--set service.nameOverride=heapster,rbac.create=true' }

#####################
## Metrics-Server ###
#####################
    - { name: metrics-server, repo: stable/metrics-server, namespace: monitoring, options: '--set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set image.repository={{ images_repo | default ("k8s.gcr.io") }}/metrics-server-{{ HOST_ARCH }} --set "args={--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}" ' }
# Proper fix for "--kubelet-insecure-tls" will be in k8s 1.19 https://github.com/kubernetes/kubeadm/issues/1602 ; Alternatives is manually generating and approving certs for each node: serverTLSBootstrap, which is complex. cert-manager option was not investigated.: https://github.com/kubernetes-sigs/metrics-server/issues/146#issuecomment-472655656

##################
## cert-manager ##
##################
    - { name: cert-manager, repo: jetstack/cert-manager, namespace: cert-manager, options: '--set prometheus.servicemonitor.enabled=true --set prometheus.servicemonitor.namespace=monitoring --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-controller --set webhook.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-webhook --set cainjector.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-cainjector --set http_proxy={{proxy_env.http_proxy | default ("") }},https_proxy={{proxy_env.https_proxy | default ("") }},no_proxy={{proxy_env.no_proxy | default ("") }} ' }
      #--set prometheus.servicemonitor.labels=prometheusoperator

################
## DASHBOARD ###
################
## This (v1) will be deprecated in favour of 2.0 - soon to be released
#    - { name: dashboard, repo: stable/kubernetes-dashboard, namespace: kube-system, options: '--set image.repository={{ images_repo | default ("k8s.gcr.io") }}/kubernetes-dashboard-{{ HOST_ARCH }} --set rbac.create=True,ingress.enabled=True,ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},ingress.hosts[1]={{ custom.networking.masterha_fqdn | default (groups["primary-master"][0]) }},ingress.hosts[2]={{ groups["primary-master"][0] }} --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set rbac.create=True,rbac.clusterAdminRole=True --set enableInsecureLogin=True --set enableSkipLogin=True ' }
# For a learning/development --set rbac.clusterAdminRole=True with skip login and insecure might be acceptable, but not for real case scenarios!!!
# For in between, one can keep: rbac.clusterReadOnlyRole=True (if bug https://github.com/helm/charts/issues/15118 was solved)
# For a production, remove --set enableInsecureLogin=True --set enableSkipLogin=True --set rbac.clusterAdminRole=True

# Option 2:
# use the below if you are sure you don't need any auth to your dashboard, and you use k8s 1.15 or older.
#    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["primary-master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},image.tag=v1.8.3 --version=0.5.3' }

####################
## DASHBOARD 2.0 ###
####################
    - { name: dashboard, repo: kubernetes-dashboard/kubernetes-dashboard, namespace: monitoring, options: '--set image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard --set ingress.enabled=True,ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},ingress.hosts[1]={{ custom.networking.masterha_fqdn | default (groups["primary-master"][0]) }},ingress.hosts[2]={{ groups["primary-master"][0] }} --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set metricsScraper.enabled=true,metricsScraper.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/metrics-scraper --set rbac.create=True,rbac.clusterReadOnlyRole=True --set protocolHttp=true --version 3.0.0' } # https://github.com/kubernetes/dashboard/blob/master/aio/deploy/helm-chart/kubernetes-dashboard/Chart.yaml#L17

################
## Kured #######
## For restaring nodes when needed, in sequencial and proper manner
################
    - { name: kured, repo: kured/kured, namespace: kube-system, options: '--set extraArgs.period="0h07m0s" --set image.repository={{ images_repo | default ("docker.io") }}/weaveworks/kured ' } #--version 1.5.1 https://github.com/weaveworks/kured/tree/master/charts/kured

################
## metallb #####
################
#    - { name: metallb, repo: stable/metallb, namespace: infra, options: '--set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.image.repository={{ images_repo | default ("docker.io") }}/metallb/controller --set speaker.nodeSelector."node\-role\.kubernetes\.io/infra=" --set speaker.image.repository={{ images_repo | default ("docker.io") }}/metallb/speaker --set prometheus.serviceMonitor.enabled=True --set prometheus.prometheusRule.enabled=True' }

####################
## INGRESS NGINX ###
####################
# (the new nginx ingress )
    - { name: ingress-nginx, repo: ingress-nginx/ingress-nginx, namespace: kube-system, options: '--set controller.admissionWebhooks.enabled=false --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=false --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("k8s.gcr.io") }} --set controller.image.repository={{ images_repo | default ("k8s.gcr.io") }}/ingress-nginx/controller --set controller.admissionWebhooks.patch.image.repository={{ images_repo | default ("docker.io") }}/jettech/kube-webhook-certgen --set defaultBackend.image.repository={{ images_repo | default ("k8s.gcr.io") }}/defaultbackend-{{ HOST_ARCH }} --version 3.12.0' } #https://github.com/kubernetes/ingress-nginx/blob/master/charts/ingress-nginx/Chart.yaml#L5

####################
## NGINX INGRESS ###
####################
    ### Option 1 - Daemonset & NodePort, (no prometheus Monitoring)
    #- { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=false --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("us.gcr.io") }} --set controller.image.repository=k8s-artifacts-prod/ingress-nginx/controller --set defaultBackend.image.repository={{ images_repo | default ("k8s.gcr.io") }}/defaultbackend-{{ HOST_ARCH }} --version 1.41.1' }
    ### Option 1b - Daemonset & NodePort like above, but with prometheus Monitoring => make sure prometheus chart is also enabled
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("us.gcr.io") }} --set controller.image.repository=k8s-artifacts-prod/ingress-nginx/controller --set defaultBackend.image.repository={{ images_repo | default ("k8s.gcr.io") }}/defaultbackend-{{ HOST_ARCH }} ' }
    # --set controller.service.externalTrafficPolicy="Local" # for better performance, when you have a LB for the ingress controllers (across all nodes with role label infra), add this as well, or use the useHostPort option below.

    ### Option 2 - DaemonSet & hostPort - Use it when there is only one machine
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.service.type=ClusterIP --set controller.kind=DaemonSet --set controller.daemonset.useHostPort=true --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("us.gcr.io") }} --set controller.image.repository=k8s-artifacts-prod/ingress-nginx/controller --set defaultBackend.image.repository={{ images_repo | default ("k8s.gcr.io") }}/defaultbackend-{{ HOST_ARCH }} ' }
    # https://rimusz.net/migrating-to-ingress-nginx hostPort setting was moved to: hostPort.enabled=true

    ### Option 3 - Deployment & NodePort
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443  --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("us.gcr.io") }} --set controller.image.repository=k8s-artifacts-prod/ingress-nginx/controller --set defaultBackend.image.repository={{ images_repo | default ("k8s.gcr.io") }}/defaultbackend-{{ HOST_ARCH }} --set controller.kind=Deployment ' }
# --set controller.service.externalTrafficPolicy="Local" # See notes above

####################
## ROOK.IO STORAGE #
####################
    #- { name: rook-ceph, repo: rook-release/rook-ceph, namespace: rook-ceph, options: '.image.repository={{ images_repo | default ("docker.io") }}/rook/ceph --set csi.cephcsi.image={{ images_repo | default ("quay.io") }}/cephcsi/cephcsi:v3.1.2 --set csi.registrar.image={{ images_repo | default ("k8s.gcr.io") }}/sig-storage/csi-node-driver-registrar:v2.0.1 --set csi.resizer.image {{ images_repo | default ("k8s.gcr.io") }}/sig-storage/csi-resizer:v1.0.0 --set csi.provisioner.image {{ images_repo | default ("k8s.gcr.io") }}/sig-storage/csi-provisioner:v2.0.0 --set csi.snapshotter.image {{ images_repo | default ("k8s.gcr.io") }}/sig-storage/csi-snapshotter:v3.0.0 --set csi.attacher.image {{ images_repo | default ("k8s.gcr.io") }}/sig-storage/csi-attacher:v3.0.0 '
#####
